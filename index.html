<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <style>
    .highlight {
      color: #0081B9; 
    }
    .question {
      color: #B8860B;  
    }
    .highlight-2 {
      color: #800080;
      font-weight: bold;
      font-style: italic;
    }
    .term {
      display: inline-block;
      padding: 0 5px;
      background-color: rgba(255,255,255,0.5);
      border-radius: 3px;
    }
    .equation {
      text-align: center;
      margin: 10px 0;
      padding: 10px;
      background-color: rgba(255,255,255,0.1);
      border-radius: 3px;
      font-weight: bold;
    }
  </style>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Taming Polysemanticity in LLMs: Provable Feature Recovery via
    Sparse Autoencoders</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Taming Polysemanticity in LLMs: Provable Feature Recovery via
              Sparse Autoencoders</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Siyu Chen</a><sup>*</sup>,</span>
              <span class="author-block">Heejune Sheen</a><sup>*</sup>,</span>
              <span class="author-block">Xuyuan Xiong</a><sup>‚Ä†</sup>,</span>
              <span class="author-block">Tianhao Wang</span></a><sup>¬ß</sup>,</span>
              <span class="author-block">Zhuoran Yang</a><sup>*</sup></span>
              
              </span>
                  </div>

                  <br>

                  <div class="is-size-6 publication-authors">
                    <span class="author-block"><sup>*</sup>Department of Statistics and Data Science, Yale University</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>
                  <div class="is-size-6 publication-authors">
                    <span class="author-block"><sup>‚Ä†</sup>Antai College of Economics and Management, Shanghai Jiao Tong University</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>
                  <div class="is-size-6 publication-authors">
                    <span class="author-block"><sup>¬ß</sup>Toyota Technological Institute at Chicago</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <!-- TODO: add arxiv paper id -->
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/FFishy-git/TamingSAE_GBA/tree/main/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <!-- TODO: add arxiv paper id -->

                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="title is-3">Abstract</div>
        <div class="content has-text-justified">
          <p>
            We address the challenge of theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for interpreting Large Language Models. Current SAE training methods lack mathematical guarantees and face issues like hyperparameter sensitivity. We propose a statistical framework for feature recovery that models polysemantic features as sparse mixtures of monosemantic concepts. Based on this, we develop a "bias adaptation" SAE training algorithm that dynamically adjusts network biases for optimal sparsity. We <span class="highlight">prove that this algorithm correctly recovers all monosemantic features</span> under our statistical model. Our improved variant, Group Bias Adaptation (GBA), <span class="highlight">outperforms existing methods on LLMs up to 1.5 billion parameters</span> in terms of sparsity-loss trade-off and feature consistency. This work provides the first SAE algorithm with theoretical recovery guarantees, advancing interpretable and trustworthy AI through enhanced mechanistic understanding.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- What are our contributions -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Contributions</h2>
        <div class="content has-text-justified">
          <!-- <p>
            Our work addresses these theoretical and practical issues in feature recovery with the following contributions:
          </p> -->
          <div class="box">
            <i class="fas fa-chart-bar"></i>
            <span class="highlight">A novel statistical framework</span> that rigorously formalizes feature recovery by modeling polysemantic features as sparse combinations of underlying monosemantic concepts, and establishes a precise notion of feature identifiability.
          </div>

          <div class="box">
            <i class="fas fa-tools"></i>
            <span class="highlight">An innovative SAE training algorithm</span>, <span class="highlight-2">Group Bias Adaptation (GBA)</span>, which adaptively adjusts neural network bias parameters to enforce optimal activation sparsity, allowing distinct groups of neurons to target different activation frequencies.
          </div>

          <div class="box">
            <i class="fas fa-calculator"></i>
            <span class="highlight">The first theoretical guarantee</span> proving that SAE training algorithm can provably recover all monosemantic features when the input data is sampled from our proposed statistical model.
          </div>

          <div class="box">
            <i class="fas fa-rocket"></i>
            <span class="highlight">Superior empirical performance</span> on LLMs up to 1.5B parameters, where GBA achieves the best sparsity-loss trade-off while learning more consistent features than benchmark methods.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Introduction -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            <h2 style="font-weight: bold; font-size: 20px;">WHAT IS FEATURE RECOVERY?</h2>
            To understand the task of feature recovery, let us consider the following example sentence: 
            <blockquote>
                <p>
                    <i>The detective found a <span style="background-color: rgba(255,255,0,0.25); color: black; font-weight: bold;">muddy footprint</span> near the <span style="background-color: rgba(0,128,128,0.10); color: black; font-weight: bold;">broken window</span>, leading him to suspect a <span style="text-decoration: underline;">?</span></i>
                </p>
            </blockquote>
            The sentence contains two distinct concepts: a muddy footprint and a broken window. For a trained deep neural network, the learned representations in the intermediate layers are often <span class="highlight">polysemantic</span>, meaning that they are mixture of multiple features of the underlying concepts.
            Specifically, the representation \(x\) after seeing the whole sentence may have the following form:
            <blockquote style="text-align: center;">
                \(x = h_1 \cdot\) <span class="term">
                    <span style="background-color: rgba(255,255,0,0.25);">
                        <b><i>"feature of muddy footprint"</i></b>
                    </span>
                </span>
                \( + h_2 \cdot\) <span class="term">
                    <span style="background-color: rgba(0,128,128,0.10);">
                        <b><i>"feature of broken window"</i></b>
                    </span>
                </span>
            </blockquote>
            where \(h_1\) and \(h_2\) are the nonnegative weights of the two <span class="highlight">monosemantic features</span>. After obtaining the polysemantic representation \(x\), the model can then generate the token "burglary" at the "?" position.<br>
            <blockquote>
              <b>The goal of feature recovery</b> is to recover the <span class="highlight">monosemantic features</span> underlying each concept, through training on a dataset that contains <span class="highlight">polysemantic representations</span>, which are often extracted from the intermediate layers of a trained deep neural network, e.g., the residual stream of a transformer-based model.
            </blockquote>
            While this example illustrates the intuitive notion of monosemantic features, we need a more rigorous definition to make progress. 
            Currently, researchers primarily assess monosemanticity through feature <i>interpretability</i>, i.e., how well a feature aligns with human-understandable concepts.
            However, this anthropocentric view has limitations: neural networks may process information in ways fundamentally different from human conceptual understanding. We need a more principled, mathematically grounded definition of monosemantic features that captures their essential properties independent of human interpretation. Specifically, we ask the following questions:
            
            </p>
            <div class="columns is-centered">
              <div class="column is-one-fourth">
                <div class="box has-text-centered">
                  <span class="tag is-info is-medium">Question 1</span>
                  <p class="mt-3">What kind of feature can be counted as <span class="highlight">monosemantic</span>?</p>
                </div>
              </div>
              <div class="column is-two-fifths">
                <div class="box has-text-centered">
                  <span class="tag is-info is-medium">Question 2</span>
                  <p class="mt-3">Given <span class="highlight">polysemantic</span> representations, how can we know monosemantic features are identifiable?</p>
                </div>
              </div>
              <div class="column is-one-fourth">
                <div class="box has-text-centered">
                  <span class="tag is-info is-medium">Question 3</span>
                  <p class="mt-3">How to recover the monosemantic features?</p>
                </div>
              </div>
            </div>
            <p>
              
            </p>
            </div>

            <div class="content has-text-justified">
            <p>
            <h2 style="font-weight: bold; font-size: 20px;">WHAT ARE SPARSE AUTOENCODERS?</h2>
            At the core of our feature recovery method is the Sparse Autoencoder (SAE), a neural network designed to learn sparse representations through self-reconstruction. A typical SAE architecture (with weight sharing between encoding and decoding layers) can be described as follows:
            \[
                f(x; \Theta) = \sum_{m=1}^M a_{m} \cdot w_{m} \cdot \phi\bigl( \underbrace{{w_{m}^\top (x-b_{\mathrm{pre}}) + b_{m}}}_{\displaystyle  \small\text{pre-activation}~y_m} \bigr) + b_{\mathrm{pre}}.
            \]

            <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
              <figure>
                <img src="./static/images/SAE.pdf" alt="Illustration of neuron grouping and bias adaptation">
                <figcaption style="font-size: small; text-align: left;">
                  <script type="text/javascript" async
                  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
                  </script>
                  Figure 1: <b>Sparse Autoencoder Architecture:</b> The SAE architecture with weight sharing between encoding and decoding layers.
                </figcaption>
              </figure>
            </div>

            A neuron is considered "activated" when its pre-activation y_m > 0. When activated, the neuron contributes to the output proportionally to its weight vector w_m, scaled by a_m.
            <ul>
                <li>
                üîç <b>What are SAEs?</b> Sparse Autoencoders (SAEs) are neural networks that are used to "decompose" the inner representation of another neural network (e.g., LLM) into a set of monosemantic features.
                </li>
                <li>
                  üîß<b>How are SAEs trained?</b> SAEs are trained to reconstruct intermediate representations of LLMs while ensuring that each input activates only a small number of neurons in the learned representation.
                </li>
                <li>
                ü§î <b>Why using SAEs?</b> The hypothesis is that by enforcing activation sparsity, SAEs can decompose polysemantic representations (a mixture of multiple features) into monosemantic features (which each correspond to a single interpretable concept).
                </li>
                <!-- <li> -->
                <!-- üéØ <b>The key challenge</b> is designing training algorithms that reliably recover these monosemantic features while maintaining good reconstruction quality.
                </li> -->
            </ul>
          </p>
        </div>
        <div class="content has-text-justified">
          <p>
            <h2 style="font-weight: bold; font-size: 20px;">WHAT ARE THE CHALLENGES</h2>
            We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. 
            <ul>
                <li><b>Theoretical uncertainty:</b> We lack clear feature definitions  and current SAE training algorithms <span class="question">lack theoretical guarantees</span>, making reliable feature recovery uncertain.</li>
                <li><b>Hyperparameter sensitivity:</b> Empirically, \( \ell_1 \) regularization and TopK activation methods are <span class="question">sensitive to hyperparameters</span> tuning.</li>
                <li><b>Feature inconsistency:</b> TopK activation methods produce <span class="question">inconsistent features</span> across different random seeds.</li>
            </ul>
            <h2 style="font-weight: bold; font-size: 20px;">WHAT ARE OUR GOALS</h2>
            Our research aims to address these challenges with the following objectives:
            <ul>
                <li>
                ‚úÖ<b>Theoretical foundation:</b> Develop a rigorous mathematical framework for understanding and analyzing feature recovery in SAEs.
                </li>
                <li>
                üìà <b>Algorithm design:</b> Create a simple and robust training algorithm that can reliably recover monosemantic features without extensive hyperparameter tuning, and learn more consistent features across different random seeds<sup>1</sup>.
                </li>
            </ul>
            
            Ultimately, our research aims to establish a rigorous foundation for mechanistic interpretability research, bringing us closer to realizing our interpretability dreams<sup>2</sup> of understanding the inner workings of neural networks.
            <br><br>
            <sup>1</sup><small><a href="https://arxiv.org/abs/2501.16615">Paulo, Gon√ßalo, and Nora Belrose. "Sparse Autoencoders Trained on the Same Data Learn Different Features." <u>https://arxiv.org/abs/2501.16615</u></a></small><br>
            
            <sup>2</sup><small><a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">Chris Olah. "Interpretability Dreams." <u>https://transformer-circuits.pub/2023/interpretability-dreams/index.html</u></a></small>
            <!-- Building on this framework, we introduce a new SAE training algorithm based on <span class="highlight">bias adaptation</span>, a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. 
            We theoretically <span class="highlight">prove that this algorithm correctly recovers all monosemantic features</span> when input data is sampled from our proposed statistical model. 
            Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and <span class="highlight">demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters</span>. 
            This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability. -->
                  </p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Algorithm Design -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Algorithm Motivation</h2>
        <div class="content has-text-justified">
          <p>
            To address the limitations of existing Sparse Autoencoder (SAE) training methods, we propose a new algorithm called <span class="highlight-2">Grouped Bias Adaptation (GBA)</span>. 
            Our algorithm has two main components: a <b>bias adaptation</b> subroutine that controls the activation frequency of each neuron, and a <b>neuron grouping</b> strategy that allows us to assign different target activation frequencies to different groups of neurons. 
          </p>
          <!-- <ol> -->
            <!-- <li><strong>Bias Adaptation</strong><br>
              Each neuron has a bias value that helps control when it activates. We adjust the bias dynamically based on the frequency of activation over a dataset. If a neuron is activated too frequently, we decrease its bias to reduce its activation. On the other hand, if it's rarely activated, we increase its bias to encourage more frequent activations. This helps maintain a balance between sufficient activation and avoiding "dead" neurons that don't contribute effectively.
            </li>
            <li><strong>Neuron Grouping</strong><br>
              Instead of applying the same activation frequency control to all neurons, we group neurons into distinct sets. Each group targets different activation frequencies, allowing us to better capture both common and rare features in the data. For example, neurons that capture frequently occurring features might have a higher target activation frequency, while those that capture rarer features might have a lower one.
            </li> -->
          <!-- </ol> -->
          <!-- <p>
            By combining these two strategies, GBA offers direct control over neuron activation, avoiding the need for complex tuning while ensuring that neurons are activated at appropriate frequencies for their tasks.
          </p> -->
        </div>
        
        <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
          <figure>
            <img src="./static/images/bias_adap_illus.png" alt="Illustration of neuron grouping and bias adaptation">
            <figcaption style="font-size: small; text-align: left;">
              <script type="text/javascript" async
              src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
              </script>
              Figure 1: <b>Neuron Grouping (Left):</b> Neurons are grouped and assigned different Target Activation Frequencies (TAFs).
              <b>Bias Adaptation (Right):</b> The bias adaptation mechanism shifts the neuron's bias \( b \) to only allow the neuron to be activated by data that produces a large pre-activation. The bias is shifted in a way that the neuron achieves the designated TAF across the training dataset.
            </figcaption>
          </figure>
        </div>

        <div class="content has-text-justified">
          <p>
            <!-- <h2 style="font-weight: bold; font-size: 20px;">SOME DETAILS ABOUT GBA</h2> -->
            <table class="table is-bordered is-fullwidth">
              <thead>
                <tr>
                  <th>Problem</th>
                  <th>Solution</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>\(\ell_1\) regularization method is sensitive to hyperparameter tuning.
                    <ul>
                      <li>The loss function is usually \(\mathcal{L} = \mathcal{L}_{\text{reconstruct}} + \lambda \mathcal{L}_{\text{sparsity}}\), where \(\lambda\) requires careful tuning.</li>
                      <li>The activation frequency is not controlled on the neuron level.
                      </li>
                    </ul>
                  </td>
                  <td>GBA allows us to gain control over the activation frequency more accurately on the neuron level by directly adapting the bias of each neuron.</td>
                </tr>
                <tr>
                  <td>TopK method does not account for data diversity:
                    <ul>
                      <li>The number of allowed activated neurons \(K\) for each input is fixed, while different inputs may have different number of features.</li>
                      <li>The activation frequency is not controlled on the neuron level.</li>
                    </ul>
                  </td>
                  <td>GBA can control the activation frequency more accurately on the neuron level by adapting the bias of each neuron.</td>
                </tr>
              </tbody>
            </table>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>

        <div class="content has-text-justified">
          <p>
            In this section, we evaluate the performance of our Grouped Bias Adaptation (GBA) algorithm across several key dimensions. To ensure a comprehensive understanding, we first briefly introduce the experimental setup before diving into the analysis of the three key questions.
          </p>
        </div>

        <!-- Experiment Setup Section -->
        <h3 class="title is-4">Experimental Setup</h3>
        <div class="content has-text-justified">
          <p>
            We conduct our experiments using two datasets: <code>Pile Github</code> and <code>Pile Wikipedia</code> datasets, each with the first 100k tokens. The experiments are performed on the <span class="highlight">Qwen2.5-1.5B base model</span>, where we attach a Sparse Autoencoder (SAE) to the MLP outputs of layers 2, 13, and 26. Each SAE contains <span class="highlight">66k hidden neurons</span> and operates with an input/output dimension of <span class="highlight">1536</span>.

            To ensure optimal performance, we adopt the JumpReLU activation function for all methods tested. We use <span class="highlight">100 million tokens</span> for training the SAEs, feeding the tokens through the LLM and collecting the MLP outputs at the specified layers.

            The four methods compared are:
            <ul>
              <li><code>TopK</code>: A sparse activation method that retains only the top-K activated neurons.</li>
              <li><code>L1</code>: A sparse method that uses L1 regularization for sparsity.</li>
              <li><code>BA (Bias Adaptation)</code>: A variant of GBA with a single group and hyperparameter tuning for sparsity.</li>
              <li><code>GBA (Grouped Bias Adaptation)</code>: Our proposed method, which uses multiple groups of neurons with target activation frequencies (TAFs) set geometrically without hyperparameter tuning.</li>
            </ul>
            All methods were trained using the AdamW optimizer with the same set of hyperparameters, including learning rate, weight decay, and batch size. In the GBA method, we used <span class="highlight">10 groups</span>, with the highest target frequency set to <span class="highlight">0.1</span> and the lowest set to <span class="highlight">0.001</span>.
          </p>
        </div>

        <h3 class="title is-4">Reconstruction Loss and Activation Sparsity Frontier</h3>
        <div class="content has-text-justified">
          <p>
            The first question we address is how the GBA method compares with other methods in terms of reconstruction loss and activation sparsity. The results show that GBA performs comparably to the TopK method with post-activation sparsity and outperforms it with pre-activation sparsity. Furthermore, GBA significantly outperforms the L1 and Bias Adaptation (BA) methods across all experiments.
          </p>
          <ul>
            <li><strong>Finding (1):</strong> GBA performs <span class="highlight">comparably to the best-performing benchmark</span>, <span class="highlight">TopK</span> with post-activation sparsity. In addition, GBA outperforms TopK with pre-activation sparsity. Specifically, when these methods have the same average fraction of activated neurons, GBA's reconstruction is comparable to that of TopK with post-activation sparsity and significantly better than that of TopK with pre-activation sparsity.</li>
            <li><strong>Finding (2):</strong> <span class="highlight">GBA outperforms L1</span> significantly. When they have the same average fraction of activated neurons, GBA achieves a lower reconstruction loss.</li>
            <li><strong>Finding (3):</strong> <span class="highlight">GBA outperforms BA</span> consistently across all experiments. This provides strong evidence that the grouping mechanism enhances both sparsity and reconstruction performance.</li>
          </ul>
        </div>

        <!-- Insert Figure for Reconstruction Loss vs Activation Sparsity -->
        <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
          <figure>
            <img src="./static/images/loss_sparsity.png" alt="Reconstruction loss vs activation sparsity">
            <figcaption style="font-size: small; text-align: left;">
              Figure 2: Reconstruction loss versus the average fraction of activated neurons for various methods. The GBA method shows competitive performance with the TopK method.
            </figcaption>
          </figure>
        </div>

        <h3 class="title is-4">Robustness and Nearly Tuning-Free</h3>
        <div class="content has-text-justified">
          <p>
            The second question revolves around the robustness of the GBA method to the choice of hyperparameters, such as the number of groups and target frequencies. Our ablation studies show that GBA is nearly tuning-free. As long as the Highest Target Frequency (HTF) is sufficiently high (e.g., 0.5) and the number of groups (K) is large enough (e.g., 10 or 20), GBA performs consistently well.
          </p>
          <ul>
            <li><strong>Finding (4):</strong> When HTF and LTF are properly chosen (e.g., a high HTF and a modestly low LTF), with an adequate number of groups, the GBA method achieves performance comparable to TopK, and the performance becomes largely <span class="highlight">insensitive to the specific choices of these parameters</span>.</li>
          </ul>
          <!-- <p>
            Even with variations in LTF and the number of groups, GBA maintains stable performance, making it less sensitive to hyperparameter tuning compared to methods like TopK, which requires manual tuning of the number of activated neurons.
          </p> -->
        </div>

        <!-- Insert Figure for GBA Robustness -->
        <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
          <figure>
            <img src="./static/images/group_ablation.png" alt="Ablation study on GBA robustness">
            <figcaption style="font-size: small; text-align: left;">
              Figure 3: Ablation study showing the robustness of GBA to different choices of the Highest Target Frequency (HTF) and the number of groups (K).
            </figcaption>
          </figure>
        </div>

        <h3 class="title is-4">Consistency of Recovered Features</h3>
        <div class="content has-text-justified">
          <p>
            <!-- The third question examines the consistency of the features learned by the GBA method across independent runs with different random seeds. We evaluate this using the Maximum Cosine Similarity (MCS) metric, and the results show that GBA outperforms other methods, including TopK, in terms of feature consistency. -->
            The third question examines the consistency of the features learned by the GBA method across independent runs with different random seeds. We evaluate this using the Maximum Cosine Similarity (MCS) metric, and the results show that GBA outperforms other methods, including TopK, in terms of feature consistency. The specific definition of MCS can be found in the ¬ßA.2 of the paper. Simply put, MCS represents the similarity between the ground truth feature and the learned feature, with higher values indicating better consistency.
          </p>
          <ul>
            <li><strong>Finding (5):</strong> As the TopK method is shown to be seed-dependent, it has the lowest MCS overall. Our <span class="highlight">GBA method outperforms TopK</span> in achieving a higher percentage of neurons with high MCS.</li>
            <li><strong>Finding (6):</strong> The L1 method is more consistent than TopK uniformly and more consistent than GBA in three of the four cases. However, when focusing on neurons with the top-0.05% activations, our GBA method surpasses the L1 method.</li>
          </ul>
        </div>

        <!-- Insert Figure for Feature Consistency -->
        <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
          <figure>
            <img src="./static/images/consistency.png" alt="Consistency of features across runs">
            <figcaption style="font-size: small; text-align: left;">
              Figure 4: Consistency of recovered features measured by Maximum Cosine Similarity (MCS) across three runs with different random seeds. GBA outperforms other methods in feature consistency.
            </figcaption>
          </figure>
        </div>

        <h3 class="title is-4">Additional Results</h3>
        <div class="content has-text-justified">
          <p>
            We provide additional studies on the neurons learned by the GBA and TopK methods in terms of the three metrics used above: maximum activation, Z-score, and maximum cosine similarity across different runs with different random seeds.
            These metrics are computed based on the validation part of the <code>Github</code> dataset, with the hook position at the MLP output of layer 26.
            For each neuron, we also compute the <em>activation fraction</em> (or activation rate), which is the fraction of tokens where the pre-activations of the neuron are non-negative.
          </p>
          <p>
            For each neuron, we have four metrics: maximum activation, Z-score, maximum cosine similarity, and activation fraction. We generate scatter plots by plotting the Z-score against the other three metrics. The results for GBA and TopK are shown below.
          </p>
        </div>

        <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
          <figure>
            <img src="./static/images/zscore_1.png" alt="Scatter plots for neuron properties (GBA and TopK)">
            <figcaption style="font-size: small; text-align: left;">
              Figure 5: Scatter plots illustrating neuron properties for the GBA method: Z-score versus Maximum Activation, Fraction of Non-negative Pre-Activations (i.e., activation frequency), and Maximum Cosine Similarity across different runs with different random seeds. The 66k-neuron SAE is trained on the <code>Github</code> dataset with a hook at the MLP output of layer 26.
            </figcaption>
          </figure>
        </div>

        <h3 class="title is-5">Subplots Explanation</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Left:</strong> Z-score vs Maximum Activation - This plot shows the relationship between the Z-score and maximum activation of neurons. We observe an <span class="highlight">almost linear relationship</span>, indicating that neurons with higher Z-scores also have higher maximum activations.</li>
            <li><strong>Middle:</strong> Z-score vs Activation Fraction - This plot illustrates the correlation between the Z-score and the activation fraction. Neurons with higher Z-scores tend to have a lower activation fraction, capturing more infrequent features.</li>
            <li><strong>Right:</strong> Z-score vs Maximum Cosine Similarity - This plot compares the Z-score and maximum cosine similarity across different runs. It shows that <span class="highlight">neurons with higher Z-scores tend to exhibit higher consistency in feature recovery across runs</span>.</li>
          </ul>
        </div>

        <!-- Placeholder for the second set of scatter plots for TopK neurons -->
        <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
          <figure>
            <img src="./static/images/zscore_2.png" alt="Scatter plots for TopK neurons">
            <figcaption style="font-size: small; text-align: left;">
              Figure 6: Scatter plots for the neurons trained using TopK with \( K=300 \), showing similar neuron properties.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3" style="white-space: nowrap;">
          Demo: Feature Dashboard
        </h2>
        <p class="content" style="text-align: left;">
          In this section, we present a demo of the feature dashboard for the SAE-learned features. These features are derived from the training experiments conducted using the Grouped Bias Adaptation (GBA) method, where the SAE was trained on the <code>Pile Github</code> dataset with the first 100k tokens. The features were extracted from the output of the MLP block at layer 26 of the <span class="highlight">Qwen2.5-1.5B base model</span>.
        </p>
        <!-- <p class="content" style="text-align: left;">
          The feature dashboard visualizes various aspects of the learned features, providing insights into their activations and behaviors across different runs. The features shown in this dashboard were trained under consistent experimental conditions, allowing for a detailed comparison between different neurons and their learned characteristics. For a comprehensive view of all the learned features, see the <a href="./dashboards.html">All Feature Dashboards</a> page.
        </p> -->
        <p class="content" style="text-align: left;">
          The feature dashboard visualizes various aspects of the learned features, providing insights into their activations and behaviors across different runs. One of the examples featured here corresponds to neuron 4688, which exhibits a clear bimodal activation pattern. This neuron is activated just before outputting the "class" token, indicating that it captures a distinct feature relevant to that part of the model's operation.
        </p>
        <p class="content" style="text-align: left;">
          For a comprehensive view of all the learned features, see the <a href="./dashboards.html">All Feature Dashboards</a> page.
        </p>
      </div>
    </div>
  </div>

  <div class="container is-fluid" style="padding-top: 0;">
    <div class="columns">
      <div class="column">
        <iframe
          src="./static/dashboards/feature_4688.html"
          title="Feature Dashboard Demo"
          style="
            width: 80%;
            max-width: 1600px;
            height: 800px;
            border: none;
            display: block;
            margin: 0 auto;
          ">
        </iframe>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
